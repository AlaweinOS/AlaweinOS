# MetaLibria: Tournament-Based Meta-Learning for Fast Algorithm Selection

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![AutoML 2025](https://img.shields.io/badge/AutoML-2025-green.svg)](https://2025.automl.cc/)

**MetaLibria** is a tournament-based meta-learning framework that achieves **sub-millisecond algorithm selection** (0.15ms) while maintaining competitive accuracy. By combining Swiss-system tournaments with Elo rating systems, MetaLibria delivers **1664Ã— speedup** over state-of-the-art methods like SATzilla, enabling real-time algorithm selection.

---

## ğŸ¯ Key Results

Across 5 diverse ASlib scenarios spanning **4,099 test instances** and **42 algorithms**:

- **Best Average Regret**: 0.0545 (vs. SATzilla: 0.0603, SMAC: 0.0659, AutoFolio: 0.0709)
- **Selection Speed**: 0.15ms (1664Ã— faster than SATzilla's 254ms)
- **GRAPHS-2015 Benchmark**: Rank 1/7 (0.019 regret, 54.8% top-1 accuracy)
- **CSP-2010 Benchmark**: 96.5% accuracy (tied with SMAC, but 49Ã— faster)
- **Hyperparameter Robustness**: Only n_clusters impacts performance; other parameters show Â±0.1% variation

**Paper Submitted**: AutoML Conference 2025 (March 30, 2025)

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/[anonymous]/metalibria-automl2025.git
cd metalibria-automl2025

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

### Basic Usage

```python
from libria_meta import MetaLibria
from libria_meta.feature_extractor import FeatureExtractor

# Initialize MetaLibria
selector = MetaLibria(
    n_clusters=3,          # Number of problem clusters (optimal: 3)
    n_tournament_rounds=5, # Swiss-system rounds (default: 5)
    elo_k=32.0,           # Elo update rate (default: 32)
    ucb_constant=1.0      # UCB exploration constant (default: 1.0)
)

# Train on ASlib scenario
from benchmark.aslib_loader import ASLibLoader

loader = ASLibLoader('aslib_data/GRAPHS-2015')
train_data = loader.get_train_data()

# Fit the model
selector.fit(
    features=train_data['features'],
    runtimes=train_data['runtimes'],
    algorithm_names=train_data['algorithms']
)

# Select algorithm for new instance
test_instance = loader.get_test_instance(0)
selected_algorithm = selector.select(test_instance['features'])

print(f"Selected: {selected_algorithm}")
print(f"Selection time: {selector.last_selection_time_ms:.3f}ms")
```

---

## ğŸ“Š Reproducing Paper Results

### Step 1: Download ASlib Benchmarks

```bash
bash scripts/download_aslib.sh
```

This downloads 5 ASlib scenarios:
- **SAT11-HAND**: 296 instances, 15 algorithms (SAT solving)
- **CSP-2010**: 486 instances, 6 algorithms (Constraint satisfaction)
- **GRAPHS-2015**: 1,147 instances, 9 algorithms (Graph coloring)
- **MAXSAT12-PMS**: 876 instances, 8 algorithms (MAX-SAT)
- **ASP-POTASSCO**: 1,294 instances, 4 algorithms (Answer set programming)

### Step 2: Run Phase 2 Evaluation (Main Results)

```bash
# Run all 5 scenarios with all 7 methods
PYTHONPATH=/path/to/libria-meta:$PYTHONPATH python3 benchmark/phase2_evaluation.py

# Results will be saved to:
# - results/phase2_results/phase2_results_summary.csv (aggregate metrics)
# - results/phase2_results/phase2_results_detailed.csv (per-instance results)
```

Expected runtime: **1-2 hours** (on Intel Xeon E5-2680 v4 @ 2.40GHz, 28 cores)

### Step 3: Run Ablation Studies

```bash
# Hyperparameter ablation across 4 parameters
PYTHONPATH=/path/to/libria-meta:$PYTHONPATH python3 benchmark/ablation_studies_real.py

# Results saved to results/ablation_real/:
# - ablation_n_clusters.csv
# - ablation_ucb_c.csv
# - ablation_n_rounds.csv
# - ablation_elo_k.csv
```

Expected runtime: **30-45 minutes**

### Step 4: Generate Paper Figures

```bash
# Generate all 4 figures from paper
python3 figures/generate_paper_figures.py

# Outputs:
# - figures/figure2_scenario_performance.pdf (per-scenario bar chart)
# - figures/figure3_pareto_frontier.pdf (speed vs. accuracy tradeoff)
# - figures/figure4_hyperparameter_sensitivity.pdf (ablation plots)
```

Note: Figure 1 (architecture diagram) is in `paper_latex/figures/figure1_architecture.pdf` (TikZ-generated).

---

## ğŸ“‚ Repository Structure

```
libria-meta/
â”œâ”€â”€ libria_meta/              # Core MetaLibria implementation
â”‚   â”œâ”€â”€ metalibria.py        # Main MetaLibria class
â”‚   â”œâ”€â”€ tournament.py        # Swiss-system tournament logic
â”‚   â”œâ”€â”€ elo_system.py        # Elo rating updates
â”‚   â”œâ”€â”€ clustering.py        # KMeans problem clustering
â”‚   â””â”€â”€ feature_extraction.py # Instance feature extraction
â”œâ”€â”€ baselines/                # Baseline implementations
â”‚   â”œâ”€â”€ satzilla.py          # SATzilla (cost-sensitive regression)
â”‚   â”œâ”€â”€ autofolio.py         # AutoFolio (pairwise classification)
â”‚   â”œâ”€â”€ smac_wrapper.py      # SMAC (Bayesian optimization)
â”‚   â”œâ”€â”€ hyperband_wrapper.py # Hyperband (successive halving)
â”‚   â””â”€â”€ bohb_wrapper.py      # BOHB (Hyperband + Bayesian)
â”œâ”€â”€ benchmark/                # Evaluation scripts
â”‚   â”œâ”€â”€ phase2_evaluation.py # Main results (Table 2, 3)
â”‚   â”œâ”€â”€ ablation_studies_real.py # Hyperparameter ablation
â”‚   â””â”€â”€ aslib_loader.py      # ASlib data loader
â”œâ”€â”€ figures/                  # Figure generation
â”‚   â””â”€â”€ generate_paper_figures.py
â”œâ”€â”€ paper_latex/              # LaTeX paper source
â”‚   â”œâ”€â”€ metalibria_paper.tex # Main paper (18 pages)
â”‚   â”œâ”€â”€ appendix_complete.tex # Appendix (9 tables)
â”‚   â”œâ”€â”€ references.bib       # 27 citations
â”‚   â””â”€â”€ figures/             # 4 paper figures (197 KB)
â”œâ”€â”€ tests/                    # Unit tests (29 tests, 100% pass)
â”œâ”€â”€ results/                  # Experimental results (CSV files)
â”œâ”€â”€ aslib_data/              # ASlib benchmark data (downloaded)
â””â”€â”€ requirements.txt         # Python dependencies

Total: 5,356 lines of code
```

---

## ğŸ”¬ Methodology

### Training Phase

1. **Clustering**: Partition problem space into k=3 clusters using KMeans on instance features
2. **Swiss-System Tournaments**: Run 5 rounds per cluster to rank algorithms
   - Pair algorithms with similar Elo ratings
   - Update ratings based on pairwise runtime comparisons
3. **Dual Elo Ratings**: Maintain global + cluster-specific ratings for each algorithm

### Selection Phase (0.15ms)

1. **Feature Extraction**: Extract instance features (<10 Î¼s)
2. **Cluster Assignment**: Assign instance to nearest cluster (45 Î¼s)
3. **UCB Selection**: Select algorithm with highest UCB score (105 Î¼s)
   - UCB(a) = normalize(Elo<sub>a,c</sub>) + Î»âˆš(log N / n<sub>a</sub>)

---

## ğŸ“ˆ Performance Summary

| Method | Avg Regret | Top-1 Acc | Top-3 Acc | Time (ms) |
|--------|-----------|----------|----------|----------|
| **MetaLibria (optimal)** | **0.0545** | 0.433 | 0.672 | **0.15** |
| MetaLibria (default) | 0.0586 | 0.411 | 0.669 | 0.17 |
| SATzilla | 0.0603 | **0.387** | **0.663** | 253.7 |
| SMAC | 0.0659 | 0.424 | 0.635 | 29.9 |
| AutoFolio | 0.0709 | 0.470 | 0.730 | 24.1 |
| Hyperband | 0.1016 | 0.198 | 0.611 | 0.20 |
| BOHB | 0.1016 | 0.198 | 0.611 | 1.52 |

**Key Findings**:
- MetaLibria achieves **best regret** (0.0545) with **1664Ã— speedup** over SATzilla
- Excels on graph problems (GRAPHS-2015: rank 1/7) and binary selection (CSP-2010: 96.5%)
- Only **n_clusters** hyperparameter matters; others show Â±0.1-0.2% variation

---

## ğŸ§ª Testing

Run the complete test suite:

```bash
# All 29 tests (should pass 100%)
pytest tests/ -v

# With coverage report
pytest tests/ --cov=libria_meta --cov-report=html
```

Test coverage:
- **metalibria.py**: Core selection logic (8 tests)
- **tournament.py**: Swiss-system pairing (6 tests)
- **elo_system.py**: Rating updates (5 tests)
- **clustering.py**: KMeans clustering (4 tests)
- **feature_extraction.py**: Instance features (6 tests)

---

## ğŸ“„ Paper

The complete paper is available in `paper_latex/metalibria_paper.pdf` (18 pages, 403 KB):

- **Main Text**: 5,710 words, 9 pages (within AutoML limit)
- **Figures**: 4 publication-quality figures (197 KB)
- **Tables**: 3 main tables + 9 appendix tables
- **Citations**: 27 references
- **Appendix**: Complete results, statistical tests, ablation details, reproducibility info

### Compiling the Paper

```bash
cd paper_latex

# Run full LaTeX compilation
pdflatex metalibria_paper.tex
bibtex metalibria_paper
pdflatex metalibria_paper.tex
pdflatex metalibria_paper.tex

# Output: metalibria_paper.pdf
```

---

## ğŸ”§ System Requirements

### Hardware
- **CPU**: Intel Xeon E5-2680 v4 or equivalent (28 cores recommended)
- **RAM**: 64 GB (32 GB minimum)
- **Disk**: 10 GB free space (for ASlib data + results)

### Software
- **OS**: Ubuntu 20.04 LTS (or compatible Linux)
- **Python**: 3.9.7 or higher
- **LaTeX**: TeX Live 2020+ (for paper compilation)

### Python Dependencies

Core libraries (see `requirements.txt`):
```
numpy==1.21.5
scikit-learn==1.0.2
scipy==1.7.3
pandas==1.3.5
matplotlib==3.5.1
```

---

## ğŸ“š Citation

If you use MetaLibria in your research, please cite our AutoML 2025 paper:

```bibtex
@inproceedings{metalibria2025,
  title={MetaLibria: Tournament-Based Meta-Learning for Fast Algorithm Selection},
  author={Anonymous Authors},
  booktitle={Proceedings of the AutoML Conference 2025},
  year={2025},
  address={Vancouver, Canada},
  month={September}
}
```

---

## ğŸ¤ Contributing

We welcome contributions! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style

- **Black** for formatting: `black libria_meta/ tests/`
- **Type hints**: All functions should have type annotations
- **Docstrings**: NumPy style docstrings for all public methods
- **Tests**: 100% test pass rate required

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **ASlib**: Algorithm Selection Library ([aslib.net](http://www.aslib.net/))
- **Baselines**: SATzilla, AutoFolio, SMAC, Hyperband, BOHB implementations
- **AutoML 2025**: Conference venue for this research

---

## ğŸ“§ Contact

For questions or feedback:
- Open an issue on GitHub
- Email: [anonymous for double-blind review]

**Status**: âœ… **Submitted to AutoML 2025** (March 30, 2025)

**Estimated Acceptance Probability**: 60-65% (based on novelty, rigor, and positioning)

---

## ğŸ—ºï¸ Roadmap

- [x] **Week 1-2**: Core implementation (Swiss-system + Elo + UCB)
- [x] **Week 3-4**: Baseline implementations (7 methods)
- [x] **Week 5-7**: ASlib benchmarking (5 scenarios, 4,099 instances)
- [x] **Week 8**: Ablation studies (4 hyperparameters)
- [x] **Week 9-11**: Paper writing (5,710 words)
- [x] **Week 12**: LaTeX conversion, figures, appendix, proofreading, submission
- [ ] **April-May 2025**: Await reviewer feedback
- [ ] **June 2025**: Camera-ready version (if accepted)
- [ ] **September 2025**: AutoML Conference presentation (Vancouver)

**Current Status**: ğŸ¯ **Submitted successfully! Awaiting reviews...**
